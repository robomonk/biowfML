# config.yaml

# --- GCS Paths ---
gcs_paths:
  # Your project's core buckets (outputs, logs)
  output_results_bucket: "rsabawi-hpc-sandbox-926388-genomics-results"
  tensorboard_log_bucket: "rsabawi-hpc-sandbox-926388-tensorboard-logs"

  # NEW: Your dedicated staging bucket for dynamic public data copies
  staging_bucket: "rsabawi-hpc-sandbox-926388-staging-area"

  # Base GCS FUSE mount path on Ray worker nodes.
  # This is the local directory where GCS buckets will be mounted.
  # E.g., if gs://my-bucket is mounted, it would appear as /mnt/gcs/my-bucket/
  gcs_fuse_mount_base: "/mnt/gcs/"

# --- Input Sources (Public GCS paths or other accessible locations) ---
# These define the *source* of the data that will be copied to your staging_bucket.
input_sources:
  rnaseq_chicken_ref: "gs://rnaseq-nf/data/ggal/transcript.fa"
  rnaseq_chicken_read1: "gs://rnaseq-nf/data/ggal/gut_1.fq"
  rnaseq_chicken_read2: "gs://rnaseq-nf/data/ggal/gut_2.fq"

  # Add other public dataset sources here as needed, e.g.:
  # human_ref: "gs://gcp-public-data--broad-references/hg38/v0/Homo_sapiens_assembly38.fasta"
  # human_reads_sample1_R1: "gs://your-public-reads-source/human_sample1_R1.fastq.gz"

# --- Tool Images (Docker Hub or Google Container Registry paths) ---
tool_images:
  bwa: "biocontainers/bwa:0.7.17--he4a0461_8"  # Was bwa_mem, now 'bwa'
  samtools: "biocontainers/samtools:1.16.1--h3840ad3_1" # Was samtools_sort, now 'samtools'
  picard: "broadinstitute/picard:latest" # Placeholder, replace with actual image
  gatk: "broadinstitute/gatk:latest"     # Placeholder, replace with actual image

# --- Workflow Definition (Sequence of abstract tasks) ---
workflow_definition:
  - task_id_prefix: "bwa-align"
    type: "bwa_mem"
    # Input sources for *this specific workflow step* are now references to `input_sources`
    inputs:
      ref: "${input_sources.rnaseq_chicken_ref}"
      read1: "${input_sources.rnaseq_chicken_read1}"
      read2: "${input_sources.rnaseq_chicken_read2}"
    # Output will go to a *dynamic subdirectory* within output_results_bucket
    output_prefix: "${gcs_paths.output_results_bucket}/${output_prefixes.aligned_bams}"
    produces_primary_output: True
    primary_output_key: "aligned_sam" # Added

  - task_id_prefix: "samtools-sort"
    type: "samtools_sort"
    inputs:
      input_bam: "dynamic_from_previous_task" # This will be the path from the BWA output
    output_prefix: "${gcs_paths.output_results_bucket}/${output_prefixes.aligned_bams}"
    produces_primary_output: True
    primary_output_key: "sorted_bam" # Added

# --- Output Prefixes (relative paths within output_results_bucket) ---
# These are prefixes for final results, not the staging area.
output_prefixes:
  aligned_bams: "aligned_bams/"
  multiqc_reports: "multiqc_reports/"

# --- GCP Cost Configuration ---
gcp_costs:
  cpu_per_hour: 0.031611
  ram_per_gb_hour: 0.004237

# --- RL Specific Configuration (initial placeholders) ---
rl_config:
  action_tiers:
    0: {cpu: 1, ram_gb: 4}
    1: {cpu: 2, ram_gb: 8}
    2: {cpu: 4, ram_gb: 16}

  reward_weights:
    success: 10.0
    cost: -1.0
    time: -0.5
    utilization: 0.2

  state_normalization_constants:
    max_duration_seconds: 3600
    max_cost: 0.5
    max_input_size_gb: 100
    max_cluster_load_percent: 1.0 # Assuming 0-1 scale for load
    max_attempts: 5 # Added
  historical_performance_window: 10 # Added as per previous step

# --- GCS FUSE Mount Configuration ---
gcs_fuse_mounts:
  # List all bucket names that need to be FUSE mounted by gcsfuse_startup.sh
  # These are just bucket names, not full gs:// paths.
  buckets_to_mount:
    - "rsabawi-hpc-sandbox-926388-genomics-reads"  # Example from original script
    - "rsabawi-hpc-sandbox-926388-genomics-refs"    # Example from original script
    # The python script create_ray_cluster_script.py will dynamically add bucket names
    # derived from gcs_paths (like output_results_bucket, tensorboard_log_bucket, staging_bucket)
    # to this list if they are not already present.